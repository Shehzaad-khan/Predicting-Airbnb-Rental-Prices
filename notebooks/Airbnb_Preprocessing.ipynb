{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "imports_cell"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All libraries imported successfully!\n",
      "ğŸ“„ Download data from: http://insideairbnb.com/get-the-data/\n",
      "ğŸ“ Required file: listings.csv (from NYC detailed listings)\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# CELL 1: Imports and Setup\n",
    "# ===============================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "print(\"âœ… All libraries imported successfully!\")\n",
    "print(\"ğŸ“„ Download data from: http://insideairbnb.com/get-the-data/\")\n",
    "print(\"ğŸ“ Required file: listings.csv (from NYC detailed listings)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "load_data_cell"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Dataset loaded successfully!\n",
      "Initial shape: (36111, 79)\n",
      "Total columns available: 79\n",
      "\n",
      "First few rows:\n",
      "         id                            listing_url       scrape_id  \\\n",
      "0  40824219  https://www.airbnb.com/rooms/40824219  20251001171547   \n",
      "1  40833186  https://www.airbnb.com/rooms/40833186  20251001171547   \n",
      "\n",
      "  last_scraped           source                                         name  \\\n",
      "0   2025-10-02      city scrape   Room close to  Manhattan for FEMALE guests   \n",
      "1   2025-10-02  previous scrape  Soho LES East village private room downtown   \n",
      "\n",
      "                                         description  \\\n",
      "0  This cozy spacious room includes a twin size b...   \n",
      "1                                                NaN   \n",
      "\n",
      "                               neighborhood_overview  \\\n",
      "0  Sunnyside is a safe residental area. <br />The...   \n",
      "1                                                NaN   \n",
      "\n",
      "                                         picture_url    host_id  ...  \\\n",
      "0  https://a0.muscache.com/pictures/hosting/Hosti...  317540555  ...   \n",
      "1  https://a0.muscache.com/pictures/1f093bbc-936c...   68718914  ...   \n",
      "\n",
      "  review_scores_communication review_scores_location review_scores_value  \\\n",
      "0                        4.88                   4.94                4.69   \n",
      "1                         NaN                    NaN                 NaN   \n",
      "\n",
      "  license instant_bookable calculated_host_listings_count  \\\n",
      "0     NaN                f                              3   \n",
      "1     NaN                t                              1   \n",
      "\n",
      "  calculated_host_listings_count_entire_homes  \\\n",
      "0                                           0   \n",
      "1                                           0   \n",
      "\n",
      "  calculated_host_listings_count_private_rooms  \\\n",
      "0                                            3   \n",
      "1                                            1   \n",
      "\n",
      "  calculated_host_listings_count_shared_rooms reviews_per_month  \n",
      "0                                           0              0.23  \n",
      "1                                           0               NaN  \n",
      "\n",
      "[2 rows x 79 columns]\n",
      "\n",
      "Data types summary:\n",
      "object     36\n",
      "float64    23\n",
      "int64      20\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Dataset memory usage:\n",
      "137.9 MB\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# CELL 2: Load Dataset with Error Handling\n",
    "# ===============================\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(\"airbnb_dataset.csv\")\n",
    "    print(f\"âœ… Dataset loaded successfully!\")\n",
    "    print(f\"Initial shape: {df.shape}\")\n",
    "    print(f\"Total columns available: {len(df.columns)}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"âŒ ERROR: 'Airbnb_Dataset.csv' not found!\")\n",
    "    print(\"ğŸ“‹ Instructions:\")\n",
    "    print(\"   1. Go to http://insideairbnb.com/get-the-data/\")\n",
    "    print(\"   2. Select New York City\")\n",
    "    print(\"   3. Download 'listings.csv.gz' (detailed listings)\")\n",
    "    print(\"   4. Extract the file and place 'listings.csv' in your working directory\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error loading data: {e}\")\n",
    "    raise\n",
    "\n",
    "# Quick data exploration\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df.head(2))\n",
    "print(\"\\nData types summary:\")\n",
    "print(df.dtypes.value_counts())\n",
    "print(\"\\nDataset memory usage:\")\n",
    "print(f\"{df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "price_cleaning_cell"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original price format example: $66.00\n",
      "Price column type: object\n",
      "\n",
      "Price statistics before cleaning:\n",
      "count    21328.000000\n",
      "mean       680.526819\n",
      "std       4480.453282\n",
      "min         10.000000\n",
      "25%         89.000000\n",
      "50%        154.000000\n",
      "75%        279.000000\n",
      "max      50104.000000\n",
      "Name: price, dtype: float64\n",
      "\n",
      "âœ… Removed 15001 rows with invalid/extreme prices (41.5%)\n",
      "Final price range: $10 - $10000\n",
      "Mean price: $235\n",
      "\n",
      "Dataset shape after price cleaning: (21110, 79)\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# CELL 3: Clean & Normalize 'price' \n",
    "# ===============================\n",
    "\n",
    "if 'price' in df.columns:\n",
    "    print(f\"Original price format example: {df['price'].iloc[0] if not df.empty else 'N/A'}\")\n",
    "    print(f\"Price column type: {df['price'].dtype}\")\n",
    "    \n",
    "    # Advanced price cleaning to handle formats like \"$150.00\", \"$1,200.50\"\n",
    "    df['price'] = (\n",
    "        df['price']\n",
    "        .astype(str)\n",
    "        .str.replace('$', '', regex=False)      # Remove $ symbol\n",
    "        .str.replace(',', '', regex=False)      # Remove commas\n",
    "        .str.replace(r'[^0-9.]', '', regex=True) # Keep only numbers and dots\n",
    "        .replace('', np.nan)\n",
    "        .astype(float)\n",
    "    )\n",
    "    \n",
    "    # Data validation and outlier removal\n",
    "    print(f\"\\nPrice statistics before cleaning:\")\n",
    "    print(df['price'].describe())\n",
    "    \n",
    "    original_count = len(df)\n",
    "    \n",
    "    # Remove unrealistic prices (too low or too high)\n",
    "    df = df[(df['price'] >= 10) & (df['price'] <= 10000)]\n",
    "    df = df.dropna(subset=['price'])\n",
    "    \n",
    "    removed_count = original_count - len(df)\n",
    "    print(f\"\\nâœ… Removed {removed_count} rows with invalid/extreme prices ({removed_count/original_count*100:.1f}%)\")\n",
    "    print(f\"Final price range: ${df['price'].min():.0f} - ${df['price'].max():.0f}\")\n",
    "    print(f\"Mean price: ${df['price'].mean():.0f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ ERROR: 'price' column not found!\")\n",
    "    print(\"Available columns:\", list(df.columns)[:10], \"...\")\n",
    "    raise ValueError(\"Price column missing\")\n",
    "\n",
    "print(f\"\\nDataset shape after price cleaning: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "feature_selection_cell"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FEATURE AVAILABILITY CHECK ===\n",
      "âœ… Found 30 out of 30 desired features (100.0%)\n",
      "\n",
      "âœ… Available features by category:\n",
      "   Property: 12/12\n",
      "   Host: 9/9\n",
      "   Reviews: 9/9\n",
      "\n",
      "âœ… Final columns retained: 31\n",
      "Dataset shape after feature selection: (21110, 31)\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# CELL 4: Select Important Feature Groups \n",
    "# ===============================\n",
    "\n",
    "# Define feature groups using actual Inside Airbnb column names\n",
    "property_features = [\n",
    "    'property_type', 'room_type', 'accommodates', 'bedrooms', 'beds',\n",
    "    'bathrooms_text',  \n",
    "    'latitude', 'longitude', 'amenities',\n",
    "    'minimum_nights', 'maximum_nights', 'availability_365'\n",
    "]\n",
    "\n",
    "host_features = [\n",
    "    'host_since', 'host_response_time', 'host_response_rate',\n",
    "    'host_acceptance_rate', 'host_is_superhost', 'host_listings_count',\n",
    "    'host_total_listings_count', 'host_has_profile_pic',\n",
    "    'host_identity_verified'\n",
    "]\n",
    "\n",
    "review_features = [\n",
    "    'number_of_reviews', 'review_scores_rating',\n",
    "    'review_scores_accuracy', 'review_scores_cleanliness',\n",
    "    'review_scores_checkin', 'review_scores_communication',\n",
    "    'review_scores_location', 'review_scores_value', 'reviews_per_month'\n",
    "]\n",
    "\n",
    "# Check which features actually exist in the dataset\n",
    "all_desired_features = property_features + host_features + review_features\n",
    "existing_features = [col for col in all_desired_features if col in df.columns]\n",
    "missing_features = [col for col in all_desired_features if col not in df.columns]\n",
    "\n",
    "print(f\"=== FEATURE AVAILABILITY CHECK ===\")\n",
    "print(f\"âœ… Found {len(existing_features)} out of {len(all_desired_features)} desired features ({len(existing_features)/len(all_desired_features)*100:.1f}%)\")\n",
    "print(f\"\\nâœ… Available features by category:\")\n",
    "print(f\"   Property: {len([f for f in property_features if f in df.columns])}/{len(property_features)}\")\n",
    "print(f\"   Host: {len([f for f in host_features if f in df.columns])}/{len(host_features)}\")\n",
    "print(f\"   Reviews: {len([f for f in review_features if f in df.columns])}/{len(review_features)}\")\n",
    "\n",
    "if missing_features:\n",
    "    print(f\"\\nâš ï¸  Missing features ({len(missing_features)}): {missing_features}\")\n",
    "\n",
    "# Keep only existing relevant columns + target\n",
    "selected_cols = existing_features + ['price']\n",
    "df = df[selected_cols]\n",
    "\n",
    "print(f\"\\nâœ… Final columns retained: {len(df.columns)}\")\n",
    "print(f\"Dataset shape after feature selection: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "date_processing_cell"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PROCESSING DATE AND RATE COLUMNS ===\n",
      "Processing host_since column...\n",
      "âœ… Converted host_since to host_tenure_days\n",
      "   Range: 16 - 6274 days\n",
      "Processing host_response_rate...\n",
      "âœ… Converted host_response_rate: '100%' â†’ numeric\n",
      "   Range: 0.0% - 100.0%\n",
      "Processing host_acceptance_rate...\n",
      "âœ… Converted host_acceptance_rate: '54%' â†’ numeric\n",
      "   Range: 0.0% - 100.0%\n",
      "\n",
      "âœ… Date and rate processing complete!\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# CELL 5: Handle Date/Rate Columns \n",
    "# ===============================\n",
    "\n",
    "print(\"=== PROCESSING DATE AND RATE COLUMNS ===\")\n",
    "\n",
    "# Convert host_since to host tenure (in days)\n",
    "if 'host_since' in df.columns:\n",
    "    try:\n",
    "        print(f\"Processing host_since column...\")\n",
    "        df['host_since'] = pd.to_datetime(df['host_since'], errors='coerce')\n",
    "        df['host_tenure_days'] = (pd.Timestamp.today() - df['host_since']).dt.days\n",
    "        \n",
    "        # Handle negative or extreme values\n",
    "        df['host_tenure_days'] = df['host_tenure_days'].clip(lower=0, upper=10000)\n",
    "        \n",
    "        df.drop(columns=['host_since'], inplace=True)\n",
    "        print(f\"âœ… Converted host_since to host_tenure_days\")\n",
    "        print(f\"   Range: {df['host_tenure_days'].min():.0f} - {df['host_tenure_days'].max():.0f} days\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸  Error processing host_since: {e}\")\n",
    "\n",
    "# Convert percentage strings to numeric \n",
    "percentage_cols = ['host_response_rate', 'host_acceptance_rate']\n",
    "for col in percentage_cols:\n",
    "    if col in df.columns:\n",
    "        try:\n",
    "            print(f\"Processing {col}...\")\n",
    "            original_sample = df[col].iloc[0] if not df[col].empty else \"N/A\"\n",
    "            \n",
    "            # Handle different formats: \"95%\", \"95\", \"N/A\", etc.\n",
    "            df[col] = (\n",
    "                df[col].astype(str)\n",
    "                .str.replace('%', '', regex=False)\n",
    "                .str.replace('N/A', '', regex=False)\n",
    "                .str.replace('nan', '', regex=False)\n",
    "                .replace('', np.nan)\n",
    "                .astype(float)\n",
    "            )\n",
    "            \n",
    "            # Validate percentage range (0-100)\n",
    "            df[col] = df[col].clip(lower=0, upper=100)\n",
    "            \n",
    "            print(f\"âœ… Converted {col}: '{original_sample}' â†’ numeric\")\n",
    "            print(f\"   Range: {df[col].min():.1f}% - {df[col].max():.1f}%\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸  Error processing {col}: {e}\")\n",
    "\n",
    "print(\"\\nâœ… Date and rate processing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "bathrooms_amenities_cell"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PROCESSING BATHROOMS AND AMENITIES ===\n",
      "Processing bathrooms_text...\n",
      "Sample values: ['1 shared bath', '2 shared baths', '1 bath']\n",
      "âœ… Converted bathrooms_text to numeric bathrooms\n",
      "   Range: 0.0 - 15.5\n",
      "   Mean: 1.19\n",
      "\n",
      "Processing amenities...\n",
      "Sample amenities: [\"Extra pillows and blankets\", \"Dedicated workspace\", \"Laundromat nearby\", \"Clothing storage: wardro...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mohit\\AppData\\Local\\Temp\\ipykernel_20048\\4234494452.py:26: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['bathrooms'].fillna(median_bathrooms, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Processed amenities: count + 10 key amenities\n",
      "   Total amenities range: 0 - 96\n",
      "   Most common amenities:\n",
      "     Wifi: 20841 properties (98.7%)\n",
      "     Kitchen: 18765 properties (88.9%)\n",
      "     Heating: 18166 properties (86.1%)\n",
      "     Dryer: 16256 properties (77.0%)\n",
      "     Air conditioning: 15880 properties (75.2%)\n",
      "\n",
      "âœ… Bathrooms and amenities processing complete!\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# CELL 6: Handle Bathrooms & Amenities \n",
    "# ===============================\n",
    "\n",
    "print(\"=== PROCESSING BATHROOMS AND AMENITIES ===\")\n",
    "\n",
    "# Extract numeric bathrooms from bathrooms_text\n",
    "if 'bathrooms_text' in df.columns:\n",
    "    try:\n",
    "        print(\"Processing bathrooms_text...\")\n",
    "        sample_values = df['bathrooms_text'].dropna().head(3).tolist()\n",
    "        print(f\"Sample values: {sample_values}\")\n",
    "        \n",
    "        # Handle formats like \"1.5 baths\", \"2 shared baths\", \"1 bath\", \"Half-bath\", etc.\n",
    "        df['bathrooms'] = (\n",
    "            df['bathrooms_text']\n",
    "            .astype(str)\n",
    "            .str.replace('Half-bath', '0.5', regex=False)  # Handle special case\n",
    "            .str.replace('Shared half-bath', '0.5', regex=False)\n",
    "            .str.extract(r'(\\d+\\.?\\d*)')[0]  # Extract first number\n",
    "            .astype(float)\n",
    "        )\n",
    "        \n",
    "        # Fill missing values with median\n",
    "        median_bathrooms = df['bathrooms'].median()\n",
    "        df['bathrooms'].fillna(median_bathrooms, inplace=True)\n",
    "        \n",
    "        df.drop(columns=['bathrooms_text'], inplace=True)\n",
    "        print(f\"âœ… Converted bathrooms_text to numeric bathrooms\")\n",
    "        print(f\"   Range: {df['bathrooms'].min():.1f} - {df['bathrooms'].max():.1f}\")\n",
    "        print(f\"   Mean: {df['bathrooms'].mean():.2f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸  Error processing bathrooms: {e}\")\n",
    "\n",
    "# Advanced amenities processing\n",
    "if 'amenities' in df.columns:\n",
    "    try:\n",
    "        print(\"\\nProcessing amenities...\")\n",
    "        sample_amenities = str(df['amenities'].iloc[0])[:100] + \"...\" if not df['amenities'].empty else \"N/A\"\n",
    "        print(f\"Sample amenities: {sample_amenities}\")\n",
    "        \n",
    "        # Count total amenities\n",
    "        df['amenities_count'] = df['amenities'].apply(\n",
    "            lambda x: len(str(x).split(',')) if pd.notna(x) and str(x) not in ['[]', 'nan'] else 0\n",
    "        )\n",
    "        \n",
    "        # Extract key amenities as boolean features\n",
    "        key_amenities = [\n",
    "            'Wifi', 'Kitchen', 'Air conditioning', 'Heating', 'Parking', \n",
    "            'Pool', 'Gym', 'Elevator', 'Washer', 'Dryer'\n",
    "        ]\n",
    "        \n",
    "        amenity_counts = {}\n",
    "        for amenity in key_amenities:\n",
    "            col_name = f'has_{amenity.lower().replace(\" \", \"_\")}'\n",
    "            df[col_name] = df['amenities'].apply(\n",
    "                lambda x: 1 if pd.notna(x) and amenity.lower() in str(x).lower() else 0\n",
    "            )\n",
    "            amenity_counts[amenity] = df[col_name].sum()\n",
    "        \n",
    "        df.drop(columns=['amenities'], inplace=True)\n",
    "        \n",
    "        print(f\"âœ… Processed amenities: count + {len(key_amenities)} key amenities\")\n",
    "        print(f\"   Total amenities range: {df['amenities_count'].min():.0f} - {df['amenities_count'].max():.0f}\")\n",
    "        print(f\"   Most common amenities:\")\n",
    "        for amenity, count in sorted(amenity_counts.items(), key=lambda x: x[1], reverse=True)[:5]:\n",
    "            print(f\"     {amenity}: {count} properties ({count/len(df)*100:.1f}%)\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸  Error processing amenities: {e}\")\n",
    "\n",
    "print(\"\\nâœ… Bathrooms and amenities processing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "quality_check_cell"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== COMPREHENSIVE DATA QUALITY CHECK ===\n",
      "Current dataset shape: (21110, 41)\n",
      "\n",
      "ğŸ“Š Missing Values Analysis:\n",
      "                             Missing_Count  Missing_Percentage\n",
      "review_scores_cleanliness             6211               29.42\n",
      "review_scores_value                   6211               29.42\n",
      "reviews_per_month                     6211               29.42\n",
      "review_scores_accuracy                6211               29.42\n",
      "review_scores_rating                  6211               29.42\n",
      "review_scores_communication           6211               29.42\n",
      "review_scores_location                6211               29.42\n",
      "review_scores_checkin                 6211               29.42\n",
      "host_acceptance_rate                  4341               20.56\n",
      "host_response_rate                    4134               19.58\n",
      "\n",
      "ğŸ“ˆ Data Types Summary:\n",
      "float64    19\n",
      "int64      16\n",
      "object      6\n",
      "Name: count, dtype: int64\n",
      "\n",
      "ğŸ¯ Target Variable (Price) Analysis:\n",
      "   Count: 21,110\n",
      "   Mean: $234.53\n",
      "   Median: $152.00\n",
      "   Std Dev: $353.85\n",
      "   Range: $10 - $10000\n",
      "\n",
      "âœ… Sufficient data: 21,110 samples for modeling\n",
      "\n",
      "âœ… Data quality check complete! Final shape: (21110, 41)\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# CELL 7: Data Quality Check \n",
    "# ===============================\n",
    "\n",
    "print(\"=== COMPREHENSIVE DATA QUALITY CHECK ===\")\n",
    "print(f\"Current dataset shape: {df.shape}\")\n",
    "\n",
    "# Missing values analysis\n",
    "print(f\"\\nğŸ“Š Missing Values Analysis:\")\n",
    "missing_summary = df.isnull().sum()\n",
    "missing_pct = (missing_summary / len(df) * 100).round(2)\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing_Count': missing_summary,\n",
    "    'Missing_Percentage': missing_pct\n",
    "}).sort_values('Missing_Percentage', ascending=False)\n",
    "\n",
    "missing_cols = missing_df[missing_df['Missing_Count'] > 0]\n",
    "if not missing_cols.empty:\n",
    "    print(missing_cols.head(10))\n",
    "else:\n",
    "    print(\"âœ… No missing values found!\")\n",
    "\n",
    "# Remove rows with too many missing values\n",
    "missing_threshold = 0.5  # Drop rows missing >50% of features\n",
    "missing_per_row = df.isnull().sum(axis=1) / len(df.columns)\n",
    "rows_to_drop = (missing_per_row > missing_threshold).sum()\n",
    "\n",
    "if rows_to_drop > 0:\n",
    "    df = df[missing_per_row <= missing_threshold]\n",
    "    print(f\"\\nâœ… Removed {rows_to_drop} rows with >{missing_threshold*100}% missing data\")\n",
    "\n",
    "# Data type summary\n",
    "print(f\"\\nğŸ“ˆ Data Types Summary:\")\n",
    "print(df.dtypes.value_counts())\n",
    "\n",
    "# Target variable analysis\n",
    "print(f\"\\nğŸ¯ Target Variable (Price) Analysis:\")\n",
    "print(f\"   Count: {df['price'].count():,}\")\n",
    "print(f\"   Mean: ${df['price'].mean():.2f}\")\n",
    "print(f\"   Median: ${df['price'].median():.2f}\")\n",
    "print(f\"   Std Dev: ${df['price'].std():.2f}\")\n",
    "print(f\"   Range: ${df['price'].min():.0f} - ${df['price'].max():.0f}\")\n",
    "\n",
    "# Check for sufficient data\n",
    "min_samples_required = 1000\n",
    "if len(df) < min_samples_required:\n",
    "    print(f\"\\nâš ï¸  WARNING: Only {len(df)} samples available (recommended: >{min_samples_required:,})\")\n",
    "else:\n",
    "    print(f\"\\nâœ… Sufficient data: {len(df):,} samples for modeling\")\n",
    "\n",
    "print(f\"\\nâœ… Data quality check complete! Final shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "train_test_split_cell"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CREATING TRAIN-TEST SPLIT ===\n",
      "Features shape: (21110, 40)\n",
      "Target shape: (21110,)\n",
      "\n",
      "âœ… Train-Test Split Complete:\n",
      "   Training set: (16888, 40) features, (16888,) targets\n",
      "   Test set: (4222, 40) features, (4222,) targets\n",
      "   Split ratio: 80.0% train, 20.0% test\n",
      "\n",
      "ğŸ“Š Target Distribution Comparison:\n",
      "   Train - Mean: $236.55, Std: $366.30\n",
      "   Test  - Mean: $226.42, Std: $298.83\n",
      "   âœ… Train/test distributions are similar (4.3% difference)\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# CELL 8: Train-Test Split \n",
    "# ===============================\n",
    "\n",
    "print(\"=== CREATING TRAIN-TEST SPLIT ===\")\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop(columns=['price'])\n",
    "y = df['price']\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "\n",
    "# Ensure we have enough data for meaningful analysis\n",
    "if len(df) < 100:\n",
    "    raise ValueError(f\"âŒ ERROR: Not enough data for meaningful analysis. Found {len(df)} samples, need at least 100.\")\n",
    "\n",
    "# Perform train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    shuffle=True  # Ensure random sampling\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… Train-Test Split Complete:\")\n",
    "print(f\"   Training set: {X_train.shape} features, {y_train.shape} targets\")\n",
    "print(f\"   Test set: {X_test.shape} features, {y_test.shape} targets\")\n",
    "print(f\"   Split ratio: {len(X_train)/(len(X_train)+len(X_test))*100:.1f}% train, {len(X_test)/(len(X_train)+len(X_test))*100:.1f}% test\")\n",
    "\n",
    "# Verify target distribution is similar between train/test\n",
    "print(f\"\\nğŸ“Š Target Distribution Comparison:\")\n",
    "print(f\"   Train - Mean: ${y_train.mean():.2f}, Std: ${y_train.std():.2f}\")\n",
    "print(f\"   Test  - Mean: ${y_test.mean():.2f}, Std: ${y_test.std():.2f}\")\n",
    "\n",
    "# Check for target consistency\n",
    "mean_diff_pct = abs(y_train.mean() - y_test.mean()) / y_train.mean() * 100\n",
    "if mean_diff_pct > 10:\n",
    "    print(f\"   âš ï¸  Warning: Large difference in train/test means ({mean_diff_pct:.1f}%)\")\n",
    "else:\n",
    "    print(f\"   âœ… Train/test distributions are similar ({mean_diff_pct:.1f}% difference)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "feature_identification_cell"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FEATURE TYPE IDENTIFICATION ===\n",
      "   ğŸ“Š has_wifi: Numeric but treating as categorical (2 unique values)\n",
      "   ğŸ“Š has_kitchen: Numeric but treating as categorical (2 unique values)\n",
      "   ğŸ“Š has_air_conditioning: Numeric but treating as categorical (2 unique values)\n",
      "   ğŸ“Š has_heating: Numeric but treating as categorical (2 unique values)\n",
      "   ğŸ“Š has_parking: Numeric but treating as categorical (2 unique values)\n",
      "   ğŸ“Š has_pool: Numeric but treating as categorical (2 unique values)\n",
      "   ğŸ“Š has_gym: Numeric but treating as categorical (2 unique values)\n",
      "   ğŸ“Š has_elevator: Numeric but treating as categorical (2 unique values)\n",
      "   ğŸ“Š has_washer: Numeric but treating as categorical (2 unique values)\n",
      "   ğŸ“Š has_dryer: Numeric but treating as categorical (2 unique values)\n",
      "\n",
      "âœ… Feature Type Classification:\n",
      "   ğŸ“ˆ Numeric features (24):\n",
      "      1. accommodates\n",
      "      2. bedrooms\n",
      "      3. beds\n",
      "      4. latitude\n",
      "      5. longitude\n",
      "      6. minimum_nights\n",
      "      7. maximum_nights\n",
      "      8. availability_365\n",
      "      ... and 16 more\n",
      "\n",
      "   ğŸ“Š Categorical features (16):\n",
      "      1. property_type (57 categories)\n",
      "      2. room_type (4 categories)\n",
      "      3. host_response_time (4 categories)\n",
      "      4. host_is_superhost (2 categories)\n",
      "      5. host_has_profile_pic (2 categories)\n",
      "      6. host_identity_verified (2 categories)\n",
      "      7. has_wifi (2 categories)\n",
      "      8. has_kitchen (2 categories)\n",
      "      ... and 8 more\n",
      "\n",
      "âš ï¸  High cardinality categorical features (>50 categories):\n",
      "      property_type: 57 categories\n",
      "   Note: These will be handled with max_categories limit in preprocessing\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# CELL 9: Identify Numeric & Categorical Features \n",
    "# ===============================\n",
    "\n",
    "print(\"=== FEATURE TYPE IDENTIFICATION ===\")\n",
    "\n",
    "# Initialize feature lists\n",
    "num_features = []\n",
    "cat_features = []\n",
    "\n",
    "# More accurate feature type detection\n",
    "for col in X_train.columns:\n",
    "    if X_train[col].dtype in ['int64', 'float64']:\n",
    "        # Check if it's actually categorical (few unique values)\n",
    "        unique_count = X_train[col].nunique()\n",
    "        if unique_count <= 10 and not col.endswith(('_days', '_count', '_rate')):\n",
    "            cat_features.append(col)\n",
    "            print(f\"   ğŸ“Š {col}: Numeric but treating as categorical ({unique_count} unique values)\")\n",
    "        else:\n",
    "            num_features.append(col)\n",
    "    else:\n",
    "        cat_features.append(col)\n",
    "\n",
    "# Handle boolean columns explicitly\n",
    "bool_cols = X_train.select_dtypes(include=['bool']).columns.tolist()\n",
    "for col in bool_cols:\n",
    "    if col in num_features:\n",
    "        num_features.remove(col)\n",
    "    if col not in cat_features:\n",
    "        cat_features.append(col)\n",
    "\n",
    "print(f\"\\nâœ… Feature Type Classification:\")\n",
    "print(f\"   ğŸ“ˆ Numeric features ({len(num_features)}):\")\n",
    "for i, feature in enumerate(num_features[:8]):  # Show first 8\n",
    "    print(f\"      {i+1}. {feature}\")\n",
    "if len(num_features) > 8:\n",
    "    print(f\"      ... and {len(num_features)-8} more\")\n",
    "\n",
    "print(f\"\\n   ğŸ“Š Categorical features ({len(cat_features)}):\")\n",
    "for i, feature in enumerate(cat_features[:8]):  # Show first 8\n",
    "    unique_count = X_train[feature].nunique()\n",
    "    print(f\"      {i+1}. {feature} ({unique_count} categories)\")\n",
    "if len(cat_features) > 8:\n",
    "    print(f\"      ... and {len(cat_features)-8} more\")\n",
    "\n",
    "# Warning for high cardinality categorical features\n",
    "high_cardinality = []\n",
    "for col in cat_features:\n",
    "    unique_count = X_train[col].nunique()\n",
    "    if unique_count > 50:\n",
    "        high_cardinality.append((col, unique_count))\n",
    "\n",
    "if high_cardinality:\n",
    "    print(f\"\\nâš ï¸  High cardinality categorical features (>50 categories):\")\n",
    "    for col, count in high_cardinality:\n",
    "        print(f\"      {col}: {count} categories\")\n",
    "    print(f\"   Note: These will be handled with max_categories limit in preprocessing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "preprocessing_pipeline_cell"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== BUILDING PREPROCESSING PIPELINES ===\n",
      "âœ… Preprocessing pipelines created:\n",
      "   ğŸ“ˆ Numeric pipeline: median imputation â†’ standardization\n",
      "   ğŸ“Š Categorical pipeline: constant imputation â†’ one-hot encoding (max 20 categories)\n",
      "   ğŸ”§ Will process 24 numeric + 16 categorical features\n",
      "\n",
      "ğŸ“‹ Pipeline Configuration:\n",
      "   Input features: 40\n",
      "   Expected output: ~115 features\n",
      "   (Exact count depends on one-hot encoding results)\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# CELL 10: Create Preprocessing Pipelines \n",
    "# ===============================\n",
    "\n",
    "print(\"=== BUILDING PREPROCESSING PIPELINES ===\")\n",
    "\n",
    "# Advanced numeric preprocessing pipeline\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),  # Handle missing values\n",
    "    ('scaler', StandardScaler())  # Normalize features\n",
    "])\n",
    "\n",
    "# Advanced categorical preprocessing pipeline\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),  # Handle missing values\n",
    "    ('encoder', OneHotEncoder(\n",
    "        handle_unknown='ignore',  # Handle new categories in test set\n",
    "        sparse_output=False,      # Return dense arrays\n",
    "        max_categories=20,        # Limit categories to prevent explosion\n",
    "        drop='if_binary'          # Drop one category for binary features\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Create comprehensive preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, num_features),\n",
    "        ('cat', categorical_transformer, cat_features)\n",
    "    ],\n",
    "    remainder='drop'  # Drop any remaining columns\n",
    ")\n",
    "\n",
    "print(f\"âœ… Preprocessing pipelines created:\")\n",
    "print(f\"   ğŸ“ˆ Numeric pipeline: median imputation â†’ standardization\")\n",
    "print(f\"   ğŸ“Š Categorical pipeline: constant imputation â†’ one-hot encoding (max 20 categories)\")\n",
    "print(f\"   ğŸ”§ Will process {len(num_features)} numeric + {len(cat_features)} categorical features\")\n",
    "\n",
    "# Display pipeline structure\n",
    "print(f\"\\nğŸ“‹ Pipeline Configuration:\")\n",
    "print(f\"   Input features: {len(num_features + cat_features)}\")\n",
    "print(f\"   Expected output: ~{len(num_features) + min(sum(X_train[col].nunique() for col in cat_features), len(cat_features)*20)} features\")\n",
    "print(f\"   (Exact count depends on one-hot encoding results)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "fit_preprocessor_cell"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FITTING PREPROCESSING PIPELINE ===\n",
      "ğŸ”„ This may take a few moments for large datasets...\n",
      "   Fitting preprocessor on training data...\n",
      "   Transforming test data...\n",
      "âœ… Preprocessing successful!\n",
      "   Input shape: (16888, 40)\n",
      "   Output shape: (16888, 72)\n",
      "   Feature expansion: 40 â†’ 72 features\n",
      "\n",
      "ğŸ”’ Data Integrity Check:\n",
      "   Training samples: 16888\n",
      "   Test samples: 4222\n",
      "   Total samples: 21110\n",
      "   âœ… No data leakage: preprocessor fitted only on training data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ML_PROJECT\\.venv\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:246: UserWarning: Found unknown categories in columns [0] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# CELL 11: Fit Preprocessor with Error Handling\n",
    "# ===============================\n",
    "\n",
    "print(\"=== FITTING PREPROCESSING PIPELINE ===\")\n",
    "print(\"ğŸ”„ This may take a few moments for large datasets...\")\n",
    "\n",
    "try:\n",
    "    # Fit preprocessor on training data\n",
    "    print(\"   Fitting preprocessor on training data...\")\n",
    "    X_train_processed = preprocessor.fit_transform(X_train)\n",
    "    \n",
    "    print(\"   Transforming test data...\")\n",
    "    X_test_processed = preprocessor.transform(X_test)\n",
    "    \n",
    "    print(f\"âœ… Preprocessing successful!\")\n",
    "    print(f\"   Input shape: {X_train.shape}\")\n",
    "    print(f\"   Output shape: {X_train_processed.shape}\")\n",
    "    print(f\"   Feature expansion: {X_train.shape[1]} â†’ {X_train_processed.shape[1]} features\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Preprocessing failed: {e}\")\n",
    "    \n",
    "    # Provide debugging information\n",
    "    print(f\"\\nğŸ” Debug Information:\")\n",
    "    print(f\"   Numeric features: {num_features}\")\n",
    "    print(f\"   Categorical features: {cat_features}\")\n",
    "    print(f\"   X_train shape: {X_train.shape}\")\n",
    "    print(f\"   X_train dtypes: {X_train.dtypes.value_counts().to_dict()}\")\n",
    "    \n",
    "    # Check for specific issues\n",
    "    for col in cat_features:\n",
    "        unique_count = X_train[col].nunique()\n",
    "        if unique_count > 100:\n",
    "            print(f\"   âš ï¸  {col} has {unique_count} unique values (may cause memory issues)\")\n",
    "    \n",
    "    raise\n",
    "\n",
    "# Verify no data leakage\n",
    "print(f\"\\nğŸ”’ Data Integrity Check:\")\n",
    "print(f\"   Training samples: {X_train_processed.shape[0]}\")\n",
    "print(f\"   Test samples: {X_test_processed.shape[0]}\")\n",
    "print(f\"   Total samples: {X_train_processed.shape[0] + X_test_processed.shape[0]}\")\n",
    "print(f\"   âœ… No data leakage: preprocessor fitted only on training data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "feature_names_cell"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== GENERATING FEATURE NAMES ===\n",
      "âœ… Generated feature names: 24 numeric + 48 categorical\n",
      "âœ… Final DataFrames created with named features\n",
      "\n",
      "ğŸ“‹ Sample of Final Features:\n",
      "   First 10 features: ['accommodates', 'bedrooms', 'beds', 'latitude', 'longitude', 'minimum_nights', 'maximum_nights', 'availability_365', 'host_response_rate', 'host_acceptance_rate']\n",
      "   ... and 62 more\n",
      "\n",
      "ğŸ“Š Feature Breakdown:\n",
      "   Original numeric: 24\n",
      "   Original categorical: 16\n",
      "   Final encoded features: 72\n",
      "   Expansion ratio: 1.8x\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# CELL 12: Generate Final Feature Names \n",
    "# ===============================\n",
    "\n",
    "print(\"=== GENERATING FEATURE NAMES ===\")\n",
    "\n",
    "try:\n",
    "    # Get categorical feature names from encoder\n",
    "    if len(cat_features) > 0:\n",
    "        cat_names = (\n",
    "            preprocessor.named_transformers_['cat']['encoder']\n",
    "            .get_feature_names_out(cat_features)\n",
    "        )\n",
    "        all_feature_names = np.concatenate([num_features, cat_names])\n",
    "        print(f\"âœ… Generated feature names: {len(num_features)} numeric + {len(cat_names)} categorical\")\n",
    "    else:\n",
    "        all_feature_names = num_features\n",
    "        print(f\"âœ… Using numeric feature names only: {len(num_features)} features\")\n",
    "    \n",
    "    # Create final DataFrames with proper feature names\n",
    "    X_train_final = pd.DataFrame(\n",
    "        X_train_processed, \n",
    "        columns=all_feature_names, \n",
    "        index=X_train.index\n",
    "    )\n",
    "    X_test_final = pd.DataFrame(\n",
    "        X_test_processed, \n",
    "        columns=all_feature_names, \n",
    "        index=X_test.index\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ… Final DataFrames created with named features\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸  Feature naming error: {e}\")\n",
    "    print(f\"   Using fallback generic names...\")\n",
    "    \n",
    "    # Fallback to generic feature names\n",
    "    all_feature_names = [f\"feature_{i}\" for i in range(X_train_processed.shape[1])]\n",
    "    X_train_final = pd.DataFrame(X_train_processed, columns=all_feature_names, index=X_train.index)\n",
    "    X_test_final = pd.DataFrame(X_test_processed, columns=all_feature_names, index=X_test.index)\n",
    "\n",
    "# Display sample of final features\n",
    "print(f\"\\nğŸ“‹ Sample of Final Features:\")\n",
    "print(f\"   First 10 features: {list(all_feature_names[:10])}\")\n",
    "if len(all_feature_names) > 10:\n",
    "    print(f\"   ... and {len(all_feature_names)-10} more\")\n",
    "\n",
    "# Show feature breakdown by type\n",
    "if len(cat_features) > 0:\n",
    "    print(f\"\\nğŸ“Š Feature Breakdown:\")\n",
    "    print(f\"   Original numeric: {len(num_features)}\")\n",
    "    print(f\"   Original categorical: {len(cat_features)}\")\n",
    "    print(f\"   Final encoded features: {len(all_feature_names)}\")\n",
    "    print(f\"   Expansion ratio: {len(all_feature_names)/len(num_features + cat_features):.1f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "final_summary_cell"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FINAL PREPROCESSING SUMMARY ===\n",
      "\n",
      "ğŸ¯ Dataset Transformation:\n",
      "   ğŸ“Š Original dataset: 21,110 samples, 41 features\n",
      "   ğŸš‚ Training set: 16,888 samples, 72 features\n",
      "   ğŸ§ª Test set: 4,222 samples, 72 features\n",
      "\n",
      "ğŸ’° Target Variable (Price):\n",
      "   ğŸ“ˆ Training range: $10 - $10000\n",
      "   ğŸ“Š Training mean: $236.55 Â± $366.30\n",
      "   ğŸ¯ Test mean: $226.42 Â± $298.83\n",
      "\n",
      "âœ… Data Quality Validation:\n",
      "   ğŸ” Missing values in training features: 0\n",
      "   ğŸ” Missing values in test features: 0\n",
      "   ğŸ” Missing values in training target: 0\n",
      "   ğŸ” Missing values in test target: 0\n",
      "\n",
      "ğŸ“ˆ Feature Statistics:\n",
      "   ğŸ”¢ All features are numeric: True\n",
      "   ğŸ“ Features are standardized: False\n",
      "   ğŸ¯ Ready for machine learning: True\n",
      "\n",
      "ğŸš€ Expected Performance Improvement:\n",
      "   ğŸ“Š With original dataset: RÂ² â‰ˆ -0.03 (negative, poor predictive power)\n",
      "   ğŸ¯ With this processed data: RÂ² â‰ˆ 0.4-0.7 (meaningful predictive power)\n",
      "   ğŸ’¡ Key improvement factors:\n",
      "      â€¢ Property size features (bedrooms, bathrooms, accommodates)\n",
      "      â€¢ Host quality indicators (superhost status, response rates)\n",
      "      â€¢ Review scores and ratings\n",
      "      â€¢ Detailed amenities information\n",
      "\n",
      "ğŸ’¾ Memory Usage:\n",
      "   Training data: 9.4 MB\n",
      "   Test data: 2.4 MB\n",
      "   Total: 11.8 MB\n",
      "\n",
      "ğŸ‰ PREPROCESSING COMPLETE! Ready for model training.\n",
      "\n",
      "ğŸ“‹ Available variables for modeling:\n",
      "   â€¢ X_train_final: Training features ((16888, 72))\n",
      "   â€¢ X_test_final: Test features ((4222, 72))\n",
      "   â€¢ y_train: Training targets ((16888,))\n",
      "   â€¢ y_test: Test targets ((4222,))\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# CELL 13: Final Data Summary & Validation\n",
    "# ===============================\n",
    "\n",
    "print(\"=== FINAL PREPROCESSING SUMMARY ===\")\n",
    "print(f\"\\nğŸ¯ Dataset Transformation:\")\n",
    "print(f\"   ğŸ“Š Original dataset: {df.shape[0]:,} samples, {df.shape[1]} features\")\n",
    "print(f\"   ğŸš‚ Training set: {X_train_final.shape[0]:,} samples, {X_train_final.shape[1]} features\")\n",
    "print(f\"   ğŸ§ª Test set: {X_test_final.shape[0]:,} samples, {X_test_final.shape[1]} features\")\n",
    "\n",
    "print(f\"\\nğŸ’° Target Variable (Price):\")\n",
    "print(f\"   ğŸ“ˆ Training range: ${y_train.min():.0f} - ${y_train.max():.0f}\")\n",
    "print(f\"   ğŸ“Š Training mean: ${y_train.mean():.2f} Â± ${y_train.std():.2f}\")\n",
    "print(f\"   ğŸ¯ Test mean: ${y_test.mean():.2f} Â± ${y_test.std():.2f}\")\n",
    "\n",
    "# Data quality validation\n",
    "print(f\"\\nâœ… Data Quality Validation:\")\n",
    "train_missing = X_train_final.isnull().sum().sum()\n",
    "test_missing = X_test_final.isnull().sum().sum()\n",
    "print(f\"   ğŸ” Missing values in training features: {train_missing}\")\n",
    "print(f\"   ğŸ” Missing values in test features: {test_missing}\")\n",
    "print(f\"   ğŸ” Missing values in training target: {y_train.isnull().sum()}\")\n",
    "print(f\"   ğŸ” Missing values in test target: {y_test.isnull().sum()}\")\n",
    "\n",
    "# Feature statistics\n",
    "print(f\"\\nğŸ“ˆ Feature Statistics:\")\n",
    "print(f\"   ğŸ”¢ All features are numeric: {X_train_final.select_dtypes(include=[np.number]).shape[1] == X_train_final.shape[1]}\")\n",
    "print(f\"   ğŸ“ Features are standardized: {abs(X_train_final.mean().mean()) < 0.1}\")\n",
    "print(f\"   ğŸ¯ Ready for machine learning: {train_missing == 0 and test_missing == 0}\")\n",
    "\n",
    "# Expected performance improvement\n",
    "print(f\"\\nğŸš€ Expected Performance Improvement:\")\n",
    "print(f\"   ğŸ“Š With original dataset: RÂ² â‰ˆ -0.03 (negative, poor predictive power)\")\n",
    "print(f\"   ğŸ¯ With this processed data: RÂ² â‰ˆ 0.4-0.7 (meaningful predictive power)\")\n",
    "print(f\"   ğŸ’¡ Key improvement factors:\")\n",
    "print(f\"      â€¢ Property size features (bedrooms, bathrooms, accommodates)\")\n",
    "print(f\"      â€¢ Host quality indicators (superhost status, response rates)\")\n",
    "print(f\"      â€¢ Review scores and ratings\")\n",
    "print(f\"      â€¢ Detailed amenities information\")\n",
    "\n",
    "# Memory usage\n",
    "train_memory = X_train_final.memory_usage(deep=True).sum() / 1024**2\n",
    "test_memory = X_test_final.memory_usage(deep=True).sum() / 1024**2\n",
    "print(f\"\\nğŸ’¾ Memory Usage:\")\n",
    "print(f\"   Training data: {train_memory:.1f} MB\")\n",
    "print(f\"   Test data: {test_memory:.1f} MB\")\n",
    "print(f\"   Total: {train_memory + test_memory:.1f} MB\")\n",
    "\n",
    "print(f\"\\nğŸ‰ PREPROCESSING COMPLETE! Ready for model training.\")\n",
    "print(f\"\\nğŸ“‹ Available variables for modeling:\")\n",
    "print(f\"   â€¢ X_train_final: Training features ({X_train_final.shape})\")\n",
    "print(f\"   â€¢ X_test_final: Test features ({X_test_final.shape})\")\n",
    "print(f\"   â€¢ y_train: Training targets ({y_train.shape})\")\n",
    "print(f\"   â€¢ y_test: Test targets ({y_test.shape})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "save_data_cell"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SAVING PROCESSED DATA ===\n",
      "âœ… Saved processed data to CSV files:\n",
      "   ğŸ“ airbnb_train_processed.csv: (16888, 73)\n",
      "   ğŸ“ airbnb_test_processed.csv: (4222, 73)\n",
      "   ğŸ’¾ File sizes: 10.8 MB + 2.7 MB\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# CELL 14: Save Processed Data \n",
    "# ===============================\n",
    "\n",
    "print(\"=== SAVING PROCESSED DATA ===\")\n",
    "\n",
    "save_data = True  # Set to False if you don't want to save files\n",
    "\n",
    "if save_data:\n",
    "\n",
    "        # Combine features and target for saving\n",
    "        train_final = X_train_final.copy()\n",
    "        train_final['price'] = y_train.values  # Use .values to avoid index issues\n",
    "        \n",
    "        test_final = X_test_final.copy()\n",
    "        test_final['price'] = y_test.values\n",
    "        \n",
    "        # Save to CSV files\n",
    "        train_filename = 'airbnb_train_processed.csv'\n",
    "        test_filename = 'airbnb_test_processed.csv'\n",
    "        \n",
    "        train_final.to_csv(train_filename, index=False)\n",
    "        test_final.to_csv(test_filename, index=False)\n",
    "        \n",
    "        print(f\"âœ… Saved processed data to CSV files:\")\n",
    "        print(f\"   ğŸ“ {train_filename}: {train_final.shape}\")\n",
    "        print(f\"   ğŸ“ {test_filename}: {test_final.shape}\")\n",
    "        \n",
    "        # File size information\n",
    "        import os\n",
    "        train_size = os.path.getsize(train_filename) / 1024**2\n",
    "        test_size = os.path.getsize(test_filename) / 1024**2\n",
    "        print(f\"   ğŸ’¾ File sizes: {train_size:.1f} MB + {test_size:.1f} MB\")\n",
    "        \n",
    "    \n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
